# This is a UNIX conf file that contains all information relating to
# the HWRF configuration.  UNIX conf is used because of how easy it is
# to parse (even GrADS can do it).  The syntax:
#
# [section]
#   var = value
#
# For generation of namelists for WRF, WPS and other Fortran programs,
# we use this syntax:
#
# [section]
#   namelist.nlvar = value
#
# to set the value of namelist &namelist's nlvar variable.  Also, the
# special variable "namelist" lists additional conf sections to
# recurse into to get more namelist variables after the current conf
# section is parsed.  Any variable will only be set once: the first
# time it is seen.

[config]
# This section sets basic configuration options used by all components.  
# Several special variables in this section are set by the HWRFConfig 
# object itself, which will overwrite them if they're set in this
# file:
#   YMDHM = analysis time (201304261830 = April 26, 2013, 18:30 UTC)
#   YMDH = analysis time excluding minute (2013042618)
#   YMD = analysis time, excluding hour and minute
#   year, YYYY = analysis time's year (ie.: 2013)
#   YY = last two digits of year
#   century, CC = first two digits of year
#   month, MM = analysis time's month (ie.: 04)
#   day, DD = analysis time's day (ie.: 26)
#   hour, cyc, HH = analysis time's hour (ie.: 18)
#   minute, min = analysis time's minute (ie.: 30)

# There may be additional variables depending on what subclass (if
# any) of the HWRFConfig is used.  You must specify the mandatory EXPT
# value, which is the name of the experiment to run.

# These are set automatically by HWRFConfig._set_vars_from_ksh:
EXPT=HWRF ; overridden by NCO in system.conf.nco
# SUBEXPT={EXPT}
# storm={ENV[STORM]}
# stnum={ENV[stormenv]}
# basin1=l
# stormlat=31.5
# stormlon=-73.7
# domlat=25.0
# domlon=-75.3
stormlabel=storm{storm_num}

# The HWRFConfig class will also set the dsetfile variable if it is
# not already set:
datastore={WORKhwrf}/hwrf_state.sqlite3

# The main configuration file:
CONFhwrf={com}/{stormlabel}.conf

# HWRF forecast ensemble configuration.  Do not change these two lines:
ENS=99   ; use 99 for deterministic or if using rocoto/run_hwrf.py
ensize=0 ; leave as 0 -- the parm/hwrf_ensemble_2014.conf overrides it

GFSVER=PROD2014

[relocate]
#scrub=no   ; disable scrubbing of relocation directories
initopt=0 ; 0: full vortex initialization, 1: relocation only

[merge]
# Nothing needed here yet
# scrub=no   ; disable scrubbing of merge directory

[dir]
# Configure paths
statusfile={WORKhwrf}/{stormlabel}.{YMDH}
intercom={WORKhwrf}/intercom  ; for communicating data files between jobs
lockdir={WORKhwrf}/lock       ; lock files for post-processing
INPUThwrf=./ ; {WORKhwrf}/input    ; parent model, obs, etc.
SPINUP_DATA={WORKhwrf}/OCEAN/

geog_data={FIXhwrf}/hwrf_wps_geo/
FIXgsi={FIXhwrf}/hwrf-gsi/
FIXcrtm={FIXhwrf}/hwrf-crtm-2.2.1/

# The full path to the domain center location file, which MUST be in
# com.  It is used to determine whether a cycle has a com directory:
domlocfile={com}/{vit[stnum]:02d}{vit[basin1lc]}.{vit[YMDH]}.domain.center

# The name of the ocean status file in the com directory:
ocstatus={stormlabel}.ocean_status

# Operational name of the ocean status file:
ocstatus2=ocean_status.{vit[stormname]}{vit[stnum]:02d}{vit[basin1]}.{cycle}
# That name is not presently used because it cannot be predicted in
# advance.  That breaks ecFlow and Rocoto.

# The name of the gsi status file in the com directory:
gsistatus={stormlabel}.gsi_status

# Operational name of the gsi status file:
gsistatus2=gsi_status.{vit[stormname]}{vit[stnum]:02d}{vit[basin1lc]}.{cycle}

# HISTCHECK: File to use to check if a prior cycle exists for a given
# storm.  This only applies to the single storm HWRF.  It should not
# use the vit[] variable; instead, use oldvit[].
HISTCHECK={oldcom}/{oldvit[stnum]:02d}{oldvit[basin1lc]}.{oldvit[YMDH]}.domain.center

[exe]

wgrib={utilexec}/wgrib
cnvgrib={utilexec}/cnvgrib
grbindex={utilexec}/grbindex
mpiserial={utilexec}/mpiserial

# tar/htar/hsi: These three are not used in EMC-maintained production
# jobs since NCO maintains ksh-based archiving jobs.  When EMC runs,
# we get these from the $PATH:
tar=tar
htar=htar
hsi=hsi

# The rest of these are compiled by the HWRF sorc/ build system:

gsi={EXEChwrf}/hwrf_gsi
post={EXEChwrf}/hwrf_post
copygb={EXEChwrf}/hwrf_egrid2latlon
tave={EXEChwrf}/hwrf_tave
vint={EXEChwrf}/hwrf_vint
gettrk={EXEChwrf}/hwrf_unified_tracker
hwrf_nhc_products={EXEChwrf}/hwrf_nhc_products
hwrf_prep={EXEChwrf}/hwrf_prep
real_nmm={EXEChwrf}/hwrf_real_nmm
swcorner_dynamic={EXEChwrf}/hwrf_swcorner_dynamic
wrf={EXEChwrf}/hwrf_wrf
wrfout_newtime={EXEChwrf}/hwrf_wrfout_newtime

hwrf_metgrid_levels={EXEChwrf}/hwrf_metgrid_levels

hwrf_ocean_fcst={EXEChwrf}/hwrf_ocean_fcst
hwrf_ocean_init={EXEChwrf}/hwrf_ocean_init
hwrf_ocean_pomprep_fb={EXEChwrf}/hwrf_ocean_pomprep_fb
hwrf_ocean_pomprep_g3={EXEChwrf}/hwrf_ocean_pomprep_g3
hwrf_ocean_pomprep_id={EXEChwrf}/hwrf_ocean_pomprep_id
hwrf_ocean_pomprep_na={EXEChwrf}/hwrf_ocean_pomprep_na
hwrf_ocean_transatl06prep={EXEChwrf}/hwrf_ocean_transatl06prep
hwrf_getsst={EXEChwrf}/hwrf_getsst
hwrf_sharp_mcs_rf_l2m_rmy5={EXEChwrf}/hwrf_sharp_mcs_rf_l2m_rmy5

hwrf_wm3c={EXEChwrf}/hwrf_wm3c

hwrf_geogrid={EXEChwrf}/hwrf_geogrid
hwrf_ungrib={EXEChwrf}/hwrf_ungrib
hwrf_metgrid={EXEChwrf}/hwrf_metgrid
hwrf_3dvar={EXEChwrf}/hwrf_diffwrf_3dvar
hwrf_bin_io={EXEChwrf}/hwrf_bin_io
hwrf_merge_nest={EXEChwrf}/hwrf_merge_nest_4x_step12_3n
hwrf_trk_guess={EXEChwrf}/hwrf_create_trak_guess
hwrf_wrf_split={EXEChwrf}/hwrf_split1
hwrf_pert_ct={EXEChwrf}/hwrf_pert_ct1
hwrf_create_nest={EXEChwrf}/hwrf_create_nest_1x_10m
hwrf_create_trak_fnl={EXEChwrf}/hwrf_create_trak_fnl
hwrf_merge_nest={EXEChwrf}/hwrf_merge_nest_4x_step12_3n
hwrf_anl_4x={EXEChwrf}/hwrf_anl_4x_step2
hwrf_anl_cs={EXEChwrf}/hwrf_anl_cs_10m
hwrf_anl_bogus={EXEChwrf}/hwrf_anl_bogus_10m
hwrf_inter_2to1={EXEChwrf}/hwrf_inter_2to1
hwrf_inter_2to6={EXEChwrf}/hwrf_inter_2to6
hwrf_inter_2to2={EXEChwrf}/hwrf_inter_2to2
hwrf_inter_4to2={EXEChwrf}/hwrf_inter_4to2
hwrf_inter_4to6={EXEChwrf}/hwrf_inter_4to6

hwrf_blend_gsi={EXEChwrf}/hwrf_blend_gsi
hwrf_readtdrstmid={EXEChwrf}/hwrf_readtdrstmid
hwrf_readtdrtime={EXEChwrf}/hwrf_readtdrtime
hwrf_readtdrtrigger={EXEChwrf}/hwrf_readtdrtrigger
hwrf_rem_prepbufr_typ_in_circle={EXEChwrf}/hwrf_rem_prepbufr_typ_in_circle
hwrf_change_prepbufr_qm_in_circle={EXEChwrf}/hwrf_change_prepbufr_qm_in_circle
hwrf_change_prepbufr_qm_typ={EXEChwrf}/hwrf_change_prepbufr_qm_typ

# Executable list if you do not run make install
#gsi={HOMEhwrf}/sorc/GSI/run/gsi.exe
#post={HOMEhwrf}/sorc/UPP/bin/unipost.exe
#wgrib={HOMEhwrf}/sorc/hwrf-utilities/exec/wgrib.exe
#copygb={HOMEhwrf}/sorc/UPP/bin/copygb.exe
#cnvgrib={HOMEhwrf}/sorc/UPP/bin/cnvgrib.exe
#tave={HOMEhwrf}/sorc/gfdl-vortextracker/trk_exec/hwrf_tave.exe
#vint={HOMEhwrf}/sorc/gfdl-vortextracker/trk_exec/hwrf_vint.exe
#grbindex={HOMEhwrf}/sorc/hwrf-utilities/exec/grbindex.exe
#gettrk={HOMEhwrf}/sorc/gfdl-vortextracker/trk_exec/hwrf_gettrk.exe
#hwrf_nhc_products={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_nhc_products.exe
#hwrf_prep={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_prep.exe
#mpiserial={HOMEhwrf}/sorc/hwrf-utilities/exec/mpiserial.exe
#real_nmm={HOMEhwrf}/sorc/WRFV3/main/real_nmm.exe
#swcorner_dynamic={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_swcorner_dynamic.exe
#wrf={HOMEhwrf}/sorc/WRFV3/main/wrf.exe
#wrfout_newtime={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_wrfout_newtime.exe
#
#hwrf_ocean_fcst={HOMEhwrf}/sorc/pomtc/ocean_exec/hwrf_ocean_fcst.exe
#hwrf_ocean_init={HOMEhwrf}/sorc/pomtc/ocean_exec/hwrf_ocean_init.exe
#hwrf_ocean_pomprep_fb={HOMEhwrf}/sorc/pomtc/ocean_exec/pomprep_fbtr.xc
#hwrf_ocean_pomprep_g3={HOMEhwrf}/sorc/pomtc/ocean_exec/pomprep_gdm3.xc
#hwrf_ocean_pomprep_id={HOMEhwrf}/sorc/pomtc/ocean_exec/pomprep_idel.xc
#hwrf_ocean_pomprep_na={HOMEhwrf}/sorc/pomtc/ocean_exec/pomprep_ncda.xc
#hwrf_ocean_transatl06prep={HOMEhwrf}/sorc/pomtc/ocean_exec/transatl06prep.xc
#hwrf_getsst={HOMEhwrf}/sorc/pomtc/ocean_exec/gfdl_getsst.exe
#hwrf_sharp_mcs_rf_l2m_rmy5={HOMEhwrf}/sorc/pomtc/ocean_exec/gfdl_sharp_mcs_rf_l2m_rmy5.exe
#
#hwrf_wm3c={HOMEhwrf}/sorc/ncep-coupler/cpl_exec/hwrf_wm3c.exe
#
#hwrf_geogrid={HOMEhwrf}/sorc/WPSV3/geogrid.exe
#hwrf_ungrib={HOMEhwrf}/sorc/WPSV3/ungrib.exe
#hwrf_metgrid={HOMEhwrf}/sorc/WPSV3/metgrid.exe
#hwrf_3dvar={HOMEhwrf}/sorc/hwrf-utilities/exec/diffwrf_3dvar.exe
#hwrf_bin_io={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_bin_io.exe
#hwrf_merge_nest={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_merge_nest_4x_step12_3n.exe
#hwrf_trk_guess={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_create_trak_guess.exe
#hwrf_wrf_split={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_split1.exe
#hwrf_pert_ct={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_pert_ct1.exe
#hwrf_create_nest={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_create_nest_1x_10m.exe
#hwrf_create_trak_fnl={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_create_trak_fnl.exe
#hwrf_anl_4x={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_anl_4x_step2.exe
#hwrf_anl_cs={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_anl_cs_10m.exe
#hwrf_anl_bogus={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_anl_bogus_10m.exe
#hwrf_inter_2to1={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_inter_2to1.exe
#hwrf_inter_2to6={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_inter_2to6.exe
#hwrf_inter_2to2={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_inter_2to2.exe
#hwrf_inter_4to2={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_inter_4to2.exe
#hwrf_inter_4to6={HOMEhwrf}/sorc/hwrf-utilities/exec/hwrf_inter_4to6.exe

# -----------------------------------------------------------------------
# Preprocessing configuration.
# -----------------------------------------------------------------------

[gfsinit]
# Subsections to configure each subtask:
geogrid=geogrid
metgrid=metgrid
realinit=wrfexe
realfcst=wrfexe
wrfanl=wrfexe
wrfghost=wrfexe
post=nonsatpost
regribber=regribber
tracker=tracker

[fgat]
# Boundary conditon step:
ibdystep=10800
# FGAT hours:
FGATSTR=-3
FGATINV=3
FGATEND=3
# Subsections to configure each subtask:
geogrid=geogrid
metgrid=metgrid
realinit=wrfexe
realfcst=wrfexe
wrfanl=wrfexe
wrfghost=wrfexe
post=nonsatpost
regribber=regribber
tracker=tracker
ungrib=fgat_ungrib
prep_hybrid=fgat_prep_hybrid

[geogrid]
redirect = yes
tbl = {PARMhwrf}/hwrf_GEOGRID.TBL
namelist = wps_namelist

[ungrib]
redirect = yes
catalog = {input_catalog}
dataset = gfs
tbl = {PARMhwrf}/hwrf_Vtable_gfs2014
item2_optional=yes
item = gfs_gribA
item2 = gfs_gribB
namelist = wps_namelist
subset_grib1 = {PARMhwrf}/hwrf_global_1x1_paramlist.f00

# tbl2011: hwrf_expt.prelaunch will replace tbl with tbl2011 for 2011 storms
tbl2011 = {PARMhwrf}/hwrf_Vtable_gfs2012

[fgat_ungrib]
item2_optional=yes
item = gdas1_gribA
item2 = gdas1_gribB
namelist = wps_namelist
@inc = ungrib
dataset = gdas1

[metgrid]
redirect = yes
tbl = {PARMhwrf}/hwrf_METGRID.TBL
namelist = wps_namelist
scrub = no

[wps_namelist]
share.wrf_core = 'NMM',
geogrid.map_proj =  'rotated_ll',
geogrid.geog_data_path = "{FIXhwrf}/hwrf_wps_geo/"
geogrid.opt_geogrid_tbl_path = './'
geogrid.ref_x = 105.0,
geogrid.ref_y = 159.0,
ungrib.out_format = 'WPS',
ungrib.prefix = 'FILE',
metgrid.fg_name = 'FILE',
metgrid.opt_metgrid_tbl_path = './'
mod_levs.press_pa = 201300, 200100, 100000, 95000, 90000, 85000, 80000,
                     75000,  70000,  65000, 60000, 55000, 50000, 45000,
                     40000,  35000,  30000, 25000, 20000, 15000, 10000,
                      5000,   1000,    500,   200

[prep_hybrid]
dataset = gfs
item = gfs_sf
catalog = {input_catalog}
namelist = prep_hybrid_namelist
threads=8
imax=1440
jmax=721

[fgat_prep_hybrid]
dataset = gdas
item = gdas1_sf
@inc=prep_hybrid
namelist = prep_hybrid_namelist

[prep_hybrid_namelist]
rgrid.pola = F
rgrid.alonvt = -90.0
rgrid.polei = 0.25
rgrid.polej = 360.
rgrid.xmeshl = 0.25
rgrid.north = F
# prmfld.ntimes = 1 ; set to 1 if it is unset here
# domain.p_top_requested = 50 ; automatically set from WRF namelist
# domain.ptsgm=20000 ; automatically set from WRF namelist
# domain.levels=0.995,... ; automatically set from WRF namelist

[relocation]
tbl = {FIXhwrf}/hwrf_eta_micro_lookup.dat
redirect = true

[bufrprep]
# scrub=no   ; disable scrubbing of bufrprep directory
catalog = {input_catalog}
obstypes = tdr_new_obstype
bufr_item=gfs_bufr
bufr_dataset=gfs
prepbufr_item=gfs_prepbufr_nr
prepbufrprep=3
RMALL=no
RRADC=50.
RBLDC=-200.

# -----------------------------------------------------------------------
# GSI CONFIGURATTION
# -----------------------------------------------------------------------

[gsi_d02]
# scrub=no   ; disable scrubbing of gsi_d02 directory
redirect=yes
catalog = {input_catalog}
use_newradbc=yes
obstypes = hdob_obstype,sat_obstypes,tdr_new_obstype
bufr_item=gfs_bufr
bufr_dataset=gfs
prepbufr_item=gfs_prepbufr_nr
nml_file={PARMhwrf}/hwrf_gsi.nml
nml_section=gsi_d02_nml
diagpre={com}/{out_prefix}.gsi_d02

[gsi_d03]
# scrub=no   ; disable scrubbing of gsi_d03 directory
redirect=yes
catalog = {input_catalog}
use_hwrf_ensemble = yes
sat_da = no
#use_gfs_stratosphere = no
use_newradbc = yes
obstypes = hdob_obstype,sat_obstypes,tdr_new_obstype
bufr_item=gfs_bufr
bufr_dataset=gfs
prepbufr_item=gfs_prepbufr_nr
nml_file={PARMhwrf}/hwrf_gsi.nml
nml_section=gsi_d03_nml
diagpre={com}/{out_prefix}.gsi_d03

[gsi_d02_nml]
# Namelist settings for domain 2 (6km) GSI
HZSCL=0.25,0.5,1.0
DELTIM=1200
JCAP=62
JCAP_B=62
twind=3.0
HYBENS_REGIONAL=T
ENSEMBLE_SIZE_REGIONAL=80
HYBENS_UV_REGIONAL=T
BETA1_INV_REGIONAL=0.2
HYBENS_HOR_SCALE_REGIONAL=300
HYBENS_VER_SCALE_REGIONAL=-0.5
READIN_LOCALIZATION=F
GENERATE_ENS_REGIONAL=F
REGIONAL_ENSEMBLE_OPTION=1
PSEUDO_HYBENS=F
GRID_RATIO_ENS=1
READIN_LOCALIZATION=F
PSEUDO_HYBENS=F
MERGE_TWO_GRID_ENSPERTS=F
PWGTFLG=F
BETAFLG=F
HYBENS_ANISO_REGIONAL=F
JCAP_ENS_REGIONAL=0
JCAP_ENS_TEST_REGIONAL=0

[gsi_d03_nml]
# Namelist settings for domain 3 (2km) GSI
HZSCL=0.2,0.4,0.8
DELTIM=1200
JCAP=62
JCAP_B=62
twind=3.0
HYBENS_REGIONAL=T
ENSEMBLE_SIZE_REGIONAL=80
HYBENS_UV_REGIONAL=T
BETA1_INV_REGIONAL=0.2
HYBENS_HOR_SCALE_REGIONAL=150
HYBENS_VER_SCALE_REGIONAL=-0.5
READIN_LOCALIZATION=F
GENERATE_ENS_REGIONAL=F
REGIONAL_ENSEMBLE_OPTION=1
PSEUDO_HYBENS=F
GRID_RATIO_ENS=1
READIN_LOCALIZATION=F
MERGE_TWO_GRID_ENSPERTS=F
PWGTFLG=F
BETAFLG=F
HYBENS_ANISO_REGIONAL=F
JCAP_ENS_REGIONAL=0
JCAP_ENS_TEST_REGIONAL=0

[hdob_obstype]
type=hd_ob
dataset=hd_obs
item=gdas1_bufr
# local dir name = gfs/gdas bufr_d name
hdobbufr=hdob

[tdr_old_obstype]
type=tdr_old
dataset=tdr
item=gdas1_bufr
# local dir name = gfs/gdas bufr_d name
tldplrso=tldplr

[tdr_new_obstype]
type=tdr_new
dataset=tdr
item=gdas1_bufr
# local dir name = gfs/gdas bufr_d name
tldplrbufr=tldplr

[tdrcheck]
# Check next cycle for TDR data
catalog = {input_catalog}
dataset=tdr
item=gdas1_bufr
obstype=tldplr
tdr_flag_file={com}/{stormlabel}.run_ensda
numofcheck=4
checksecinv=600

[sat_obstypes]
type=satellite
dataset=gfs
item=gfs_bufr
# local dir name = gfs/gdas bufr_d name
satwndbufr=satwnd
gpsrobufr=gpsro
# ssmirrbufr=spssmi
# tmirrbufr=sptrmm
# gomebufr=gome
# omebufr=ome
# sbuvbufr=osbuv4
gsnd1bufr=goesfv
amsuabufr=1bamua
# amsubbufr=1bamub
# hirs3bufr=1bhrs3
hirs4bufr=1bhrs4
mhsbufr=1bmhs
airsbufr=airsev
seviribufr=sevcsr
iasibufr=mtiasi
amsuabufrears=esamua
amsubbufrears=esamub
hirs3bufrears=eshrs3
# ssmitbufr=ssmit
# amsrebufr=amsre
# ssmisbufr=ssmisu
atmsbufr=atms
crisbufr=cris

# -----------------------------------------------------------------------
# POST configuration.
# -----------------------------------------------------------------------

[forecast_products]
# Forecast output frequency in seconds
wrf_output_step=10800
pom_output_step=86400

# Post-processing start, end and step for various components:
tracker_step=1
nonsatpost_step=1
satpost_step=6
wrfcopier_start=0
wrfcopier_end=9
wrfcopier_step=3
combinetrack_fhr=12

# Settings for GRIB1 grid 255 for each grid:
d23_grid=0.02,0.02,12.,15.,136,751,601
d123low_grid=0.20,0.20,90.,110.,136,551,451
d123high_grid=0.06,0.06,90.,110.,136,1834,1500
d2_grid=0.06,0.06,12.,14.,136,234,201
d3_grid=0.02,0.02,7.5,9.0,136,450,375
d2t_grid=0.06,0.06,30.,30.,128,500,500
d1t_grid=0.10,0.10,30.,30.,128,301,301

# Tracker grid expansion settings:
trk_expand=2.5,2.5,4.0,4.0,1000,1000,128,0.02,0.02

# GRIB2 compression method
grib2_compression=32  ; complex packing with second-order differences
# grib2_compression=40   ; "lossless" jpeg 2000

# Output filenames:
hwrftrk%com={out_prefix}.hwrftrk.f{fahr:03d}.grb
hwrftrk%intercom={out_prefix}.hwrftrk.grbf{fahr:02d}

anl_outer={out_prefix}.wrfanl_d02
anl_inner={out_prefix}.wrfanl_d03

hwrfprs_m%intercom={out_prefix}.hwrfprs.d23.0p02.f{fahr:03d}.grb
hwrfprs_n%intercom={out_prefix}.hwrfprs.d3.0p02.f{fahr:03d}.grb
hwrfprs_i%intercom={out_prefix}.hwrfprs.d2.0p06.f{fahr:03d}.grb
hwrfprs_p%intercom={out_prefix}.hwrfprs.d1.0p20.f{fahr:03d}.grb
hwrfprs_c%intercom={out_prefix}.hwrfprs.d123.0p06.f{fahr:03d}.grb
hwrfprs_g%intercom={out_prefix}.hwrfprs.d123.0p25.f{fahr:03d}.grb

hwrfsat_m%intercom={out_prefix}.hwrfsat.d23.0p02.f{fahr:03d}.grb
hwrfsat_n%intercom={out_prefix}.hwrfsat.d3.0p02.f{fahr:03d}.grb
hwrfsat_i%intercom={out_prefix}.hwrfsat.d2.0p06.f{fahr:03d}.grb
hwrfsat_p%intercom={out_prefix}.hwrfsat.d1.0p20.f{fahr:03d}.grb
hwrfsat_c%intercom={out_prefix}.hwrfsat.d12.0p06.f{fahr:03d}.grb
hwrfsat_g%intercom={out_prefix}.hwrfsat.d123.0p25.f{fahr:03d}.grb

hwrf2prs_m%com={out_prefix}.hwrfprs.d23.0p02.f{fahr:03d}.grb2
hwrf2prs_n%com={out_prefix}.hwrfprs.d3.0p02.f{fahr:03d}.grb2
hwrf2prs_i%com={out_prefix}.hwrfprs.d2.0p06.f{fahr:03d}.grb2
hwrf2prs_p%com={out_prefix}.hwrfprs.d1.0p20.f{fahr:03d}.grb2
hwrf2prs_c%com={out_prefix}.hwrfprs.d123.0p06.f{fahr:03d}.grb2
hwrf2prs_g%com={out_prefix}.hwrfprs.d123.0p25.f{fahr:03d}.grb2

hwrf2sat_m%com={out_prefix}.hwrfsat.d23.0p02.f{fahr:03d}.grb2
hwrf2sat_n%com={out_prefix}.hwrfsat.d3.0p02.f{fahr:03d}.grb2
hwrf2sat_i%com={out_prefix}.hwrfsat.d2.0p06.f{fahr:03d}.grb2
hwrf2sat_p%com={out_prefix}.hwrfsat.d1.0p20.f{fahr:03d}.grb2
hwrf2sat_c%com={out_prefix}.hwrfsat.d12.0p06.f{fahr:03d}.grb2
hwrf2sat_g%com={out_prefix}.hwrfsat.d123.0p25.f{fahr:03d}.grb2

[gsi_products]
# Settings for GRIB1 grid 255 for each grid:
d3_grid=0.02,0.02,12.,12.,136,600,600
d2_grid=0.06,0.06,30.,30.,136,500,500

# GRIB2 compression method
grib2_compression=32  ; complex packing with second-order differences
# grib2_compression=40   ; "lossless" jpeg 2000

# Delivery settings:
hwrforg_n%com={out_prefix}.hwrforg_n.grb2f00
hwrforg_i%com={out_prefix}.hwrforg_i.grb2f00
hwrfges_n%com={out_prefix}.hwrfges_n.grb2f00
hwrfges_i%com={out_prefix}.hwrfges_i.grb2f00
hwrfanl_n%com={out_prefix}.hwrfanl_n.grb2f00
hwrfanl_i%com={out_prefix}.hwrfanl_i.grb2f00

[copywrf]
# Nothing needed here now

[nonsatpost]
auxhist2_control={PARMhwrf}/hwrf_cntrl.tracker
control={PARMhwrf}/hwrf_cntrl.nonsat

[gsipost]
# GSI post uses the regular non-satellite post control file.
control={PARMhwrf}/hwrf_cntrl.nonsat
needcrtm=no  ; do not link CRTM fix files

[gsigribber]
# Configure the regribber/gribtask for the GSI.  Should be identical
# to the [regribber].
@inc=regribber

[satpost]
control={PARMhwrf}/hwrf_cntrl.sat{basin1}

[regribber]
griblockdir={WORKhwrf}/regribber/lock
hgt_levs={PARMhwrf}/hwrf_hgt_levs.txt
tmp_levs={PARMhwrf}/hwrf_tmp_levs.txt

[tracker]
namelist=trackernml

[trackerd02]
namelist=trackernml,nothermo

[trackerd01]
namelist=trackernml,nothermo

[nothermo]
# tracker namelist overrides to disable thermodynamics parameters:
phaseinfo.phaseflag='n'
structinfo.structflag='n'
structinfo.ikeflag='n'

[trackernml]
# tracker namelist settings:
datein.inp%model=17
fnameinfo.gmodname="hwrf"
fnameinfo.rundescr="20x20"
atcfinfo.atcfnum=81
atcfinfo.atcfname='HWRF'
trackerinfo.trkrinfo%mslpthresh=0.0015
trackerinfo.trkrinfo%v850thresh=1.5000
trackerinfo.trkrinfo%contint=100.0
phaseinfo.wcore_depth=1.0
phaseinfo.phasescheme='both'
waitinfo.wait_max_wait=3600

[nhc_products]
# Configuration for hwrf_nhc_products program, which produces the
# swath, the HTCF, the AFOS, and various other NHC products.
TierI_model=HWRF    ; model name in Tier I file
TierI_submodel=PYHW ; submodel name in Tier I file
TierI_realtime=no   ; realtime/non-realtime for Tier I purposes
grads_byteswap=yes  ; should the grads ctl file say the swath is byteswapped?
swathres=0.05  ; resolution of storm swath in degrees
swathpad=0.3   ; padding in degrees around storm swath

# -----------------------------------------------------------------------
# POM configuration.
# -----------------------------------------------------------------------

[pom]
catalog = {input_catalog}

sfc_dataset=gfs
sanl_item=gfs_sanl
sfcanl_item=gfs_sfcanl

loop_dataset=loopdata
loop_item=gfdl_loop
wc_ring_item=gfdl_wc_ring

# -----------------------------------------------------------------------
# WRF configuration.
# -----------------------------------------------------------------------

# This configuration file does not set start/end times, I/O
# configuration, locations or task geometry.  All of that is set in
# the Python code.  However, everything else is set here.  In each
# section, the special variable "namelist" tells the parser to recurse
# into a list of conf sections to get more namelist variables.
# Sections later in the list will be parsed first.  Any variable will
# only be set once: the first time it is seen.

[wrf]
# Set a few variables that the Python code must be aware of to
# communicate correctly between HWRF components.  These will also be
# used to calculate or set the appropriate WPS and WRF namelist values:
dt = 38+4/7
bdystep = 21600
ptsgm = 15000
ptop = 200
prep_hybrid=.true.
#metgrid_soil_levels=2
# default io_form, which can be overridden
io_form=11 ; NOTE: auxinput1/2 set in [wrf_namelist]
# Set the WRF namelist values that are NOT per domain:
namelist = wrf_namelist

[wrfexe]
# This section is read by all scripts that run the wrf or nmm_real.
# Fix file locations:
fix.eta_lookup={FIXhwrf}/hwrf_eta_micro_lookup.dat
fix.track={FIXhwrf}/hwrf_track
fix.wrf_other={FIXhwrf}/hwrf-wrf/*
cpl_nml=cpl_nml
dt_c=540    ; coupler timestep in seconds
sleeptime=30  ; sleep time between checks of child process

[runwrf]
# This section is read by the script that runs the WRF forecast job.
# It is not used by other executions of wrf.
sleeptime=60 ; sleep time between checks of child processes
@inc=wrfexe
wm3c_ranks=4

[cpl_nml]
CPL_SETTINGS.restart=F
CPL_SETTINGS.cstepmax={cstepmax}
CPL_SETTINGS.dt_c={dt_c}

[wrf_namelist]
# This section sets WRF namelist variables that are NOT on a
# per-domain basis.

# Force NetCDF for geogrid and metgrid because PNetCDF is not supported
time_control.io_form_auxinput1=2
time_control.io_form_auxinput2=2

time_control.debug_level = 1
#domains.halo_debug = 3
physics.var_ric = 1.0
physics.coef_ric_l = 0.16
physics.coef_ric_s = 0.25
physics.co2tf = 1
physics.num_soil_layers = 4
dynamics.euler_adv = .False.
bdy_control.spec_bdy_width = 1
bdy_control.specified = .true.
domains.feedback = 1
domains.num_moves = -99
physics.pert_sas=.false.
physics.pert_pbl=.false.
physics.ens_pblamp=0.2
physics.ens_sasamp=50.0
physics.ens_random_seed={ENS}
domains.eta_levels = 1.0, 0.995253,0.990479,0.985679,0.980781,0.975782,0.970684,0.965486,0.960187,0.954689,0.948991,0.943093,0.936895,0.930397,0.923599,0.916402,0.908404,0.899507,0.888811,0.876814,0.862914,0.847114,0.829314,0.809114,0.786714,0.762114,0.735314,0.706714,0.676614,0.645814,0.614214,0.582114,0.549714,0.517114,0.484394,0.451894,0.419694,0.388094,0.356994,0.326694,0.297694,0.270694,0.245894,0.223694,0.203594,0.185494,0.169294,0.154394,0.140494,0.127094,0.114294,0.101894,0.089794,0.078094,0.066594,0.055294,0.044144,0.033054,0.022004,0.010994,0.0,
logging.compute_slaves_silent = .true.
logging.io_servers_silent = .true.
logging.stderr_logging = 0
auxhist1_outname="wrfdiag_d<domain>"
auxhist2_outname="wrfout_d<domain>_<date>"
auxhist3_outname="wrfout_d<domain>_<date>"
physics.tg_option=1
physics.icloud=3
dynamics.terrain_smoothing = 2
time_control.tg_reset_stream=1

# Disable upscale feedback smoother, which is totally unused in HWRF
# anyway.  This removes an expensive halo:
domains.smooth_option=0

# Disable I/O server polling since it isn't supported for quilt_pnc
# yet.  This is not needed; quilt_pnc automatically disables polling
# anyway, but this removes a warning message that is confusing people:
namelist_quilt%poll_servers = .false.


[moad]
# Mother Of All Domains (MOAD) in WRF terminology is the fixed,
# outermost domain in the simulation.  For all three HWRF simulations
# (anl, ghost and forecast) the MOAD is the same, and this section
# sets all settings for that domain.  The hwrf.wrf.WRFNamelist class
# copies the MOAD settings to the child domain unless the child
# overrides them (with a few exceptions like parent_grid_ratio), so
# these sections affect all domains in all WRF simulations.
nx = 288
ny = 576
parent_grid_ratio = 1
dx = 0.135
dy = 0.135
start = moad
namelist = moad_namelist

[storm1outer]
nx = 142
ny = 274
parent_grid_ratio = 3
start = auto
namelist = namelist_outer

[storm1inner]
nx = 265
ny = 472
parent_grid_ratio = 3
start = centered
istart = 27
jstart = 58
namelist = namelist_inner

[storm1ghost]
nx = 435
ny = 868
start = centered
parent_grid_ratio = 3
istart = 28
jstart = 54
namelist = namelist_inner,namelist_ghost

[storm1ghost_parent]
nx = 290
ny = 580
parent_grid_ratio = 3
start = auto
istart = 38
jstart = 81
namelist = namelist_outer,namelist_ghost

[storm1ghost_big]
nx = 529
ny = 998
start = centered
istart = 36
jstart = 59
parent_grid_ratio = 3
namelist = namelist_inner,namelist_ghost

[storm1ghost_parent_big]
nx = 280
ny = 546
parent_grid_ratio = 3
start = auto
istart = 101
jstart = 206
namelist = namelist_outer,namelist_ghost

[namelist_ghost]
# Nothing needed here yet.

[namelist_outer]
# This section sets the namelist for the 6km and 2km domains.  It only
# needs to set values that differ from the MOAD.  The 2km domains can
# override these values in namelist_inner.
physics.nrads = 84
physics.nradl = 84
physics.nphs = 6
physics.ncnvc = 6
physics.gwd_opt = 0
physics.movemin = 7
physics.sas_pgcon = 0.2
physics.nomove_freq = 6.0
dynamics.coac = 3.0
physics.ntornado=5
physics.nomove_freq=6
domains.coral_y=18
domains.coral_x=9

[namelist_inner]
# This section sets the namelist for the 2km domains.  It only needs
# to set values that differ from the 6km domains.
physics.nrads = 252
physics.nradl = 252
physics.movemin = 14
physics.vortex_tracker = 7
dynamics.coac = 4.0
physics.ntornado=15
physics.cu_physics=0
domains.coral_y=16
domains.coral_x=8

[moad_namelist]
# This section contains namelist information that are specified for
# the WRF MOAD.  Note that some namelist variables (io servers, io,
# domain size, location, parentage) are set automatically by the
# Python code.  See hwrf/wrf.py for details.
physics.mp_physics = 5
physics.ra_lw_physics = 4
physics.ra_sw_physics = 4
physics.sf_sfclay_physics = 88
physics.sf_surface_physics = 2
physics.bl_pbl_physics = 3
physics.cu_physics = 84
physics.mommix = 1.0
physics.h_diff = 1.0
physics.gwd_opt = 2
physics.sfenth = 0.0
physics.nrads = 28
physics.nradl = 28
physics.nphs = 2
physics.ncnvc = 2
physics.gfs_alpha = -1.0
physics.sas_pgcon = 0.55
physics.vortex_tracker = 2
physics.movemin = 7
physics.ntornado = 2
physics.sas_mass_flux=0.5
dynamics.non_hydrostatic = .true.
dynamics.wp = 0
dynamics.coac = 0.75
dynamics.codamp = 6.4
physics.nomove_freq=0

physics.icoef_sf=2
physics.lcurr_sf=.false.

domains.coral_x=6
domains.coral_y=6

# ----------------------------------------------------------------------
# HWRF Ensemble DA Settings
# ----------------------------------------------------------------------
[enswrf]
namelist=wrf

[ensdadom]
ny = 702        ; roughly 30x30 degrees
nx = 350
start = auto
parent_grid_ratio = 3
namelist = namelist_outer,namelist_ghost

[ensda_prep_hybrid]
namelist = prep_hybrid_namelist
dataset = enkf
item = enkf_sfg
anl_item = enkf_siganl
catalog = {input_catalog}
threads = 8

[hwrf_da_ens]
fcsttask=ensda_runwrf
realinit=wrfexe
prep_hybrid=ensda_prep_hybrid
ensda_size=40    ; number of ensemble members (from 30 to 80)

[ensda_runwrf]
simlen=21600
nproc_x=-1
nproc_y=-1
nio_groups=1
nio_tasks_per_group=0
@inc=runwrf,wrfexe

# ----------------------------------------------------------------------
# Delivery settings
# ----------------------------------------------------------------------
